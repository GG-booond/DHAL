# T1 Robot Configuration using Hydra
defaults:
  - _self_

# Environment settings
env:
  num_envs: 4096
  n_scan: 17 * 11
  n_priv: 3 + 3 + 3
  n_priv_latent: 4 + 1 + 12 + 12
  n_proprio: 2 + 3 + 3 + 3 + 46 + 1  # 58 total
  n_recon_num: 2 + 3 + 3 + 23 + 23 + 1
  history_len: 20
  num_observations: ${eval:'${env.history_len} * ${env.n_proprio}'}  # 1160
  num_privileged_obs: 1630
  num_actions: 23
  env_spacing: 3.0
  send_timeouts: true
  episode_length_s: 20
  obs_type: "og"
  randomize_start_pos: false
  randomize_start_vel: false
  randomize_start_yaw: false
  rand_yaw_range: 1.2
  randomize_start_y: false
  rand_y_range: 0.5
  randomize_start_pitch: false
  rand_pitch_range: 1.6
  contact_buf_len: 100
  next_goal_threshold: 0.2
  reach_goal_delay: 0.1
  num_future_goal_obs: 2
  num_contact: 2  # Bipedal

# Normalization
normalization:
  obs_scales:
    lin_vel: 2.0
    ang_vel: 0.25
    dof_pos: 1.0
    dof_vel: 0.05
    height_measurements: 5.0
  clip_observations: 100.0
  clip_actions: 3.0

# Noise
noise:
  add_noise: true
  noise_level: 1.0
  noise_scales:
    imu: 0.08
    base_ang_vel: 0.4
    gravity: 0.05
    dof_pos: 0.05
    dof_vel: 0.1

# Terrain (simplified)
terrain:
  mesh_type: 'plane'
  curriculum: true
  num_goals: 8

# Commands
commands:
  curriculum: false
  resampling_time: 10.0
  lin_vel_clip: 0.2
  ang_vel_clip: 0.2
  lin_vel_x: [-1.0, 1.0]
  lin_vel_y: [-1.0, 1.0]
  ang_vel_yaw: [-1.0, 1.0]
  body_height_cmd: [-0.05, 0.05]

# Init state
init_state:
  pos: [0.0, 0.0, 0.68]
  rot: [0.0, 0.0, 0.0, 1.0]
  lin_vel: [0.0, 0.0, 0.0]
  ang_vel: [0.0, 0.0, 0.0]
  default_joint_angles:
    AAHead_yaw: 0.0
    Head_pitch: 0.0
    Left_Shoulder_Pitch: 0.2
    Left_Shoulder_Roll: -1.35
    Left_Elbow_Pitch: 0.0
    Left_Elbow_Yaw: -0.5
    Right_Shoulder_Pitch: 0.2
    Right_Shoulder_Roll: 1.35
    Right_Elbow_Pitch: 0.0
    Right_Elbow_Yaw: 0.5
    Waist: 0.0
    Left_Hip_Pitch: -0.2
    Left_Hip_Roll: 0.0
    Left_Hip_Yaw: 0.0
    Left_Knee_Pitch: 0.4
    Left_Ankle_Pitch: -0.25
    Left_Ankle_Roll: 0.0
    Right_Hip_Pitch: -0.2
    Right_Hip_Roll: 0.0
    Right_Hip_Yaw: 0.0
    Right_Knee_Pitch: 0.4
    Right_Ankle_Pitch: -0.25
    Right_Ankle_Roll: 0.0

# Control
control:
  control_type: 'P'
  stiffness: {'joint': 40.0}
  damping: {'joint': 1.0}
  action_scale: 0.25
  decimation: 4

# Asset
asset:
  file: '${oc.env:LEGGED_GYM_ROOT_DIR}/resources/robots/t1/urdf/t1.urdf'
  foot_name: "foot_link"
  penalize_contacts_on: ["Trunk", "Shank", "Ankle"]
  actuated_dof_names: ['AAHead_yaw', 'Head_pitch',
                      'Left_Shoulder_Pitch', 'Left_Shoulder_Roll', 'Left_Elbow_Pitch', 'Left_Elbow_Yaw', 
                      'Right_Shoulder_Pitch', 'Right_Shoulder_Roll', 'Right_Elbow_Pitch', 'Right_Elbow_Yaw', 
                      'Waist',
                      'Left_Hip_Pitch', 'Left_Hip_Roll', 'Left_Hip_Yaw', 'Left_Knee_Pitch', 'Left_Ankle_Pitch', 'Left_Ankle_Roll',
                      'Right_Hip_Pitch', 'Right_Hip_Roll', 'Right_Hip_Yaw', 'Right_Knee_Pitch', 'Right_Ankle_Pitch', 'Right_Ankle_Roll']
  terminate_after_contacts_on: ["Trunk", "Shank", "Ankle"]

# Rewards
rewards:
  scales:
    tracking_lin_vel: 1.6
    tracking_ang_vel: 0.8
    hip_pos: 0.6
    orientation: -2.0
    dof_acc: -2.5e-7
    collision: -1.0
    action_rate: -0.22
    delta_torques: -1.0e-7
    torques: -0.00001
    lin_vel_z: -0.1
    ang_vel_xy: -0.01
    reg_orientation: -25.0
  cycle_time: 4
  only_positive_rewards: true
  base_height_target: 0.5

# Sim
sim:
  dt: 0.005
  substeps: 1
  gravity: [0.0, 0.0, -9.81]
  up_axis: 1

# PPO Training
ppo:
  seed: -1
  runner:
    policy_class_name: 'ActorCriticMLP'
    algorithm_class_name: 'PPO_HDS'
    num_steps_per_env: 24
    max_iterations: 100000
    save_interval: 300
    experiment_name: 't1'
    run_name: ''
    resume: false
    load_run: -1
    checkpoint: -1
  algorithm:
    value_loss_coef: 1.0
    use_clipped_value_loss: true
    clip_param: 0.2
    entropy_coef: 0.01
    num_learning_epochs: 5
    num_mini_batches: 4
    learning_rate: 2.0e-4
    schedule: 'adaptive'
    gamma: 0.99
    lam: 0.9
    desired_kl: 0.01
    max_grad_norm: 1.0
    glide_advantage_w: 0.35
    push_advantage_w: 0.4
    sim2real_advantage_w: 0.25
  policy:
    continue_from_last_std: true
    actor_hidden_dims: [512, 256, 128]
    critic_hidden_dims: [512, 256, 128]
    dha_hidden_dims: [256, 64, 32]
    num_modes: 3
    tsdyn_hidden_dims: [256, 128, 64]
    tsdyn_latent_dims: 20
    rnn_hidden_size: 512
    rnn_num_layers: 1